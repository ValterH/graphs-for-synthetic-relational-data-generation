{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(G, graph=False):\n",
    "    \"\"\"\n",
    "    Computes summary statistics for a given graph G.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    G : NetworkX graph object\n",
    "        The input graph.\n",
    "    graph : bool, optional (default=False)\n",
    "        Whether to plot the degree distribution of the graph or not.\n",
    "    \"\"\"\n",
    "    if nx.number_connected_components(G) != 1:\n",
    "         print(\"Number of connected components: \", nx.number_connected_components(G))\n",
    "    else:\n",
    "        print(\"Graph is connected.\")\n",
    "    print(\"Number of nodes: \", G.number_of_nodes())\n",
    "    print(\"Number of edges: \", G.number_of_edges())\n",
    "\n",
    "    if G.is_directed():\n",
    "        in_degree = 0\n",
    "        out_degree = 0\n",
    "        for node in G.nodes():\n",
    "            in_degree += G.in_degree(node)\n",
    "            out_degree += G.out_degree(node)\n",
    "        print(\"Average in-degree: \", in_degree/G.number_of_nodes())\n",
    "        print(\"Average out-degree: \", out_degree/G.number_of_nodes())\n",
    "        print(\"Maximum in-degree: \", max(dict(G.in_degree()).values()), \"for node\", max(dict(G.in_degree()).items(), key=lambda x: x[1])[0])\n",
    "    else:\n",
    "        print(\"Maximum degree: \", max(dict(G.degree()).values()), \"for node\", max(dict(G.degree()).items(), key=lambda x: x[1])[0])\n",
    "\n",
    "    print(\"Average degree: \", 2*G.number_of_edges()/G.number_of_nodes())\n",
    "    print(\"Density: \", nx.density(G))\n",
    "    print(\"Average clustering coefficient: \", nx.average_clustering(G))\n",
    "    degree_counts = Counter(dict(G.degree()).values())\n",
    "\n",
    "    if graph:\n",
    "        plt.loglog()\n",
    "        plt.scatter(degree_counts.keys(), degree_counts.values())\n",
    "        plt.xlabel(\"k\")\n",
    "        plt.ylabel(\"p_k\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "SEED = 42\n",
    "#FILE_ABS_PATH = pathlib.Path(__file__) # absolute path of this file\n",
    "###########################################################################################\n",
    "\n",
    "# TODO:\n",
    "# 1. add features to all the nodes\n",
    "# 2. add the option to return the dataset as one big graph or a list of disconnected graphs\n",
    "# 3. ...\n",
    "\n",
    "\n",
    "# get a dataframe with source and target columns\n",
    "# the columns contain the id's of the nodes in the graph\n",
    "def tables_to_graph(df, source, target, directed=True):\n",
    "    \n",
    "    # generate a graph from the dataframe\n",
    "    create_using=nx.DiGraph() if directed else nx.Graph()\n",
    "    G = nx.from_pandas_edgelist(df, source=source, target=target, create_using=create_using)\n",
    "    \n",
    "    # TODO: features (if the df is a join of the two tables we can do it by specifying the source and target columns we want for features)\n",
    "    # TODO: disconnected graphs (figure out if we want strongly/weakly connected graphs or just all the reachable nodes from the source node in the parent table)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "# read in rossmann data and convert it to a graph\n",
    "def rossman_to_graph(dir_path, train=True):\n",
    "    store_df = pd.read_csv(dir_path / \"store.csv\")\n",
    "    sales_df = pd.read_csv(dir_path / \"train.csv\") if train else pd.read_csv(dir_path / \"test.csv\")\n",
    "    \n",
    "    store_id_mapping = {store_id: i for i, store_id in enumerate(store_df[\"Store\"].unique())}\n",
    "    sales_id_mapping = {sales_id: i + len(store_id_mapping) for i, sales_id in enumerate(sales_df.index)}\n",
    "    \n",
    "    store_sales_df = pd.DataFrame()\n",
    "    store_sales_df[\"Store\"] = sales_df[\"Store\"].map(store_id_mapping)\n",
    "    store_sales_df[\"Sale\"] = sales_df.index.map(sales_id_mapping)\n",
    "    \n",
    "    root_nodes = store_sales_df[\"Store\"].unique().tolist()\n",
    "    G = tables_to_graph(store_sales_df, source=\"Store\", target=\"Sale\")\n",
    "\n",
    "    return G, root_nodes\n",
    "\n",
    "\n",
    "# read in mutagenesis data and convert it to a graph\n",
    "def mutagenesis_to_graph(dir_path):\n",
    "    molecule_df = pd.read_csv(dir_path + \"/molecule.csv\")\n",
    "    atom_df = pd.read_csv(dir_path + \"/atom.csv\")\n",
    "    bond_df = pd.read_csv(dir_path + \"/bond.csv\")\n",
    "    \n",
    "    molecule_id_mapping = {molecule_id: i for i, molecule_id in enumerate(molecule_df[\"molecule_id\"].unique())}\n",
    "    atom_id_mapping = {atom_id: i + len(molecule_id_mapping) for i, atom_id in enumerate(atom_df[\"atom_id\"].unique())}\n",
    "    bond_id_mapping = {bond_id: i + len(molecule_id_mapping) + len(atom_id_mapping) for i, bond_id in enumerate(bond_df.index)}\n",
    "    \n",
    "    # first bipartite component\n",
    "    molecule_atom_df = pd.DataFrame()\n",
    "    molecule_atom_df[\"Molecule\"] = atom_df[\"molecule_id\"].map(molecule_id_mapping)\n",
    "    molecule_atom_df[\"Atom\"] = atom_df[\"atom_id\"].map(atom_id_mapping)\n",
    "    G_molecule_to_atom = tables_to_graph(molecule_atom_df, source=\"Molecule\", target=\"Atom\")\n",
    "\n",
    "    # Label molecules and atoms in G_molecule_to_atom\n",
    "    for node in G_molecule_to_atom.nodes():\n",
    "        if node in molecule_id_mapping.values():\n",
    "            G_molecule_to_atom.nodes[node]['y'] = 0\n",
    "        else:\n",
    "            G_molecule_to_atom.nodes[node]['y'] = 1\n",
    "\n",
    "    # second bipartite component\n",
    "    atom_bond_df = pd.DataFrame()\n",
    "    atom_bond_df[\"Atom1\"] = bond_df[\"atom1_id\"].map(atom_id_mapping)\n",
    "    atom_bond_df[\"Atom2\"] = bond_df[\"atom2_id\"].map(atom_id_mapping)\n",
    "    atom_bond_df[\"Bond\"] = bond_df.index.map(bond_id_mapping)\n",
    "    # connect atom1 to bond\n",
    "    G_atom1_to_bond = tables_to_graph(atom_bond_df, source=\"Atom1\", target=\"Bond\")\n",
    "    # connect atom2 to bond\n",
    "    G_atom2_to_bond = tables_to_graph(atom_bond_df, source=\"Atom2\", target=\"Bond\")\n",
    "    # combine the two graphs\n",
    "    G_atom_to_bond = nx.compose(G_atom1_to_bond, G_atom2_to_bond)\n",
    "\n",
    "    # Label atoms and bonds in G_atom_to_bond\n",
    "    for node in G_atom_to_bond.nodes():\n",
    "        if node in atom_id_mapping.values():\n",
    "            G_atom_to_bond.nodes[node]['y'] = 1\n",
    "        else:\n",
    "            G_atom_to_bond.nodes[node]['y'] = 2\n",
    "    \n",
    "    \n",
    "    root_nodes = molecule_atom_df[\"Molecule\"].unique().tolist()\n",
    "    # combine the two bipartite components\n",
    "    G = nx.compose(G_molecule_to_atom, G_atom_to_bond)\n",
    "\n",
    "    return G, root_nodes\n",
    "\n",
    "\n",
    "# assume we have a parent table with 1:N relationship\n",
    "# -> we can split the graph by choosing the nodes in the parent table as root nodes and generating a tree for each root node\n",
    "# NOTE: we can imagine this as modeling the dataset tables as a list of connected rows\n",
    "def graph_to_subgraphs(G, root_nodes):\n",
    "    subgraphs = []\n",
    "    for root_node in root_nodes:\n",
    "        subgraph_nodes = nx.descendants(G, root_node)\n",
    "        subgraph_nodes.add(root_node) # add for sets works inplace\n",
    "        subgraphs.append(G.subgraph(subgraph_nodes))\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1, G1_roots = mutagenesis_to_graph(\"../../data/mutagenesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of connected components:  188\n",
      "Number of nodes:  10324\n",
      "Number of edges:  15379\n",
      "Maximum degree:  40 for node 135\n",
      "Average degree:  2.979271600154979\n",
      "Density:  0.00028860521167828915\n",
      "Average clustering coefficient:  0.0\n"
     ]
    }
   ],
   "source": [
    "# transform G1 to an undirected graph\n",
    "G1_undirected = G1.to_undirected()\n",
    "summary(G1_undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G1_components = graph_to_subgraphs(G1, G1_roots)\n",
    "# nx.draw(G1_components[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "def convert_networkx_to_pyg(graph):\n",
    "    # Convert NetworkX graph to PyG Data object\n",
    "    data = from_networkx(graph)\n",
    "\n",
    "    # Extract node features 'y' from NetworkX graph and convert to tensor\n",
    "    features = []\n",
    "    node_id_mapping = {}  # Mapping of node IDs to their indices\n",
    "    for i, (node_id, node_data) in enumerate(graph.nodes(data=True)):\n",
    "        features.append([node_data['y']])\n",
    "        node_id_mapping[node_id] = i  # Map the original node ID to its index\n",
    "    data.x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "    # Calculate and add node degrees\n",
    "    degrees = [val for _, val in graph.degree()]\n",
    "    data.y = torch.tensor(degrees, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return data, node_id_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from our graph\n",
    "data, node_mapping = convert_networkx_to_pyg(G1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GINModel, self).__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(num_features, 32), nn.ReLU(), nn.Linear(32, 32))\n",
    "        nn2 = nn.Sequential(nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, 32))\n",
    "\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.fc = nn.Linear(32, 1)  # Output layer for degree prediction\n",
    "\n",
    "    def forward(self, x, edge_index, return_embeds=False):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        if return_embeds:\n",
    "            return x  # Return embeddings after the last GINConv layer\n",
    "\n",
    "        return self.fc(x)  # Continue to the final output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 20.16192626953125\n",
      "Epoch 10, Loss: 13.418213844299316\n",
      "Epoch 20, Loss: 10.96291446685791\n",
      "Epoch 30, Loss: 8.75473690032959\n",
      "Epoch 40, Loss: 2.9852118492126465\n",
      "Epoch 50, Loss: 1.347115159034729\n",
      "Epoch 60, Loss: 1.227333664894104\n",
      "Epoch 70, Loss: 1.2706952095031738\n",
      "Epoch 80, Loss: 1.2169338464736938\n",
      "Epoch 90, Loss: 1.2114359140396118\n",
      "Epoch 100, Loss: 1.211816668510437\n",
      "Epoch 110, Loss: 1.2093065977096558\n",
      "Epoch 120, Loss: 1.2096365690231323\n",
      "Epoch 130, Loss: 1.2092987298965454\n",
      "Epoch 140, Loss: 1.2093380689620972\n",
      "Epoch 150, Loss: 1.2097351551055908\n",
      "Epoch 160, Loss: 1.213737964630127\n",
      "Epoch 170, Loss: 1.2095575332641602\n",
      "Epoch 180, Loss: 1.2093513011932373\n",
      "Epoch 190, Loss: 1.209480881690979\n"
     ]
    }
   ],
   "source": [
    "# Assuming each node has one feature\n",
    "model = GINModel(num_features=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.MSELoss()\n",
    "epochs = 200\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_func(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Extract embeddings after the final epoch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model(data.x, data.edge_index, return_embeds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_embeddings = {}\n",
    "for node_id in G1_roots:\n",
    "    node_index = node_mapping[node_id]\n",
    "    embedding = embeddings[node_index]\n",
    "    root_embeddings[node_id] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to root_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "# Convert tensor embeddings to lists for JSON serialization\n",
    "root_embeddings_serializable = {node_id: embedding.tolist() for node_id, embedding in root_embeddings.items()}\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"root_embeddings.json\"\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(root_embeddings_serializable, f)\n",
    "\n",
    "print(f\"Saved embeddings to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GINConv\n",
    "\n",
    "# class GINModel(nn.Module):\n",
    "#     def __init__(self, num_features):\n",
    "#         super(GINModel, self).__init__()\n",
    "#         nn1 = nn.Sequential(nn.Linear(num_features, 32), nn.ReLU(), nn.Linear(32, 32))\n",
    "#         nn2 = nn.Sequential(nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, 32))\n",
    "\n",
    "#         self.conv1 = GINConv(nn1)\n",
    "#         self.conv2 = GINConv(nn2)\n",
    "#         self.fc = nn.Linear(32, 1)  # Output layer for degree prediction\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.relu(self.conv2(x, edge_index))\n",
    "#         return self.fc(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
